{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#libraries\n",
    "# TensorFlow, google machine learning library\n",
    "import tensorflow as tf\n",
    "# Neural networks library\n",
    "from tensorflow import keras\n",
    "# database management library\n",
    "import pandas as pd\n",
    "# linear algebra library\n",
    "import numpy as np\n",
    "# plot library\n",
    "import matplotlib.pyplot as plt\n",
    "# scientific math library\n",
    "import scipy.io\n",
    "# machine learning library\n",
    "import sklearn as skl\n",
    "from sklearn import preprocessing\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.model_selection import KFold \n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn import preprocessing\n",
    "from sklearn.decomposition import PCA\n",
    "# pseudo-random numbers library\n",
    "import random\n",
    "# library for correlation map\n",
    "import seaborn as sns\n",
    "from sklearn.svm import SVR \n",
    "from joblib import dump,load\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('inverter Data Set.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx1 = data.iloc[:,0:-3]\n",
    "#dfy1 = data_raw[['u_a_k-1','u_b_k-1','u_c_k-1']]\n",
    "dfy1 = data.iloc[:,-3:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = preprocessing.StandardScaler()\n",
    "scaler.fit(dfx1)\n",
    "dfx1=scaler.transform(dfx1)\n",
    "dfx1 = pd.DataFrame(dfx1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "k_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_fold = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "decision_depth = 15"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = KFold(n_splits=k_fold, random_state=None)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Tree"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "k_arr = [5]\n",
    "kf_avg_mse_train=[]\n",
    "kf_avg_rmse_train=[]\n",
    "kf_avg_r2_train=[]\n",
    "kf_avg_mape_train=[]\n",
    "\n",
    "kf_avg_mse_test=[]\n",
    "kf_avg_rmse_test=[]\n",
    "kf_avg_r2_test=[]\n",
    "kf_avg_mape_test=[]\n",
    "\n",
    "kf_fold_arr=[]\n",
    "\n",
    "for k in k_arr:\n",
    "\n",
    "            kf = KFold(n_splits=k, random_state=None)\n",
    "            rt = DecisionTreeRegressor(max_depth=16, min_samples_split=2)\n",
    "\n",
    "            mse_score_train = []\n",
    "            rmse_score_train = []\n",
    "            r2_score_train = []\n",
    "            mape_score_train = []\n",
    "\n",
    "            mse_score_test = []\n",
    "            rmse_score_test = []\n",
    "            r2_score_test = []\n",
    "            mape_score_test = []\n",
    "\n",
    "            for i, (train_index , test_index) in enumerate(kf.split(dfx1)):\n",
    "                X_train , X_test = dfx1.iloc[train_index,:],dfx1.iloc[test_index,:]\n",
    "                y_train , y_test = dfy1.iloc[train_index] , dfy1.iloc[test_index]\n",
    "                \n",
    "            rt.fit(X_train,y_train)\n",
    "            y_pred_train = rt.predict(X_train)\n",
    "            y_pred_test = rt.predict(X_test)\n",
    "            dump(rt,f'DecisionTreeRegressor_2_{i}.h5')\n",
    "            \n",
    "            mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "            mse_score_train.append(mse_train)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            rmse_score_train.append(rmse_train)\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_score_train.append(r2_train)\n",
    "            mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "            mape_score_train.append(mape_train)\n",
    "            mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "            mse_score_test.append(mse_test)\n",
    "            rmse_test = np.sqrt(mse_test)\n",
    "            rmse_score_test.append(rmse_test)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            r2_score_test.append(r2_test)\n",
    "            mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "            mape_score_test.append(mape_test)\n",
    "                \n",
    "            kf_fold_arr.append(k)\n",
    "\n",
    "            kf_avg_mse_train.append(sum(mse_score_train))\n",
    "            kf_avg_rmse_train.append(sum(rmse_score_train))\n",
    "            kf_avg_r2_train.append(sum(r2_score_train))\n",
    "            kf_avg_mape_train.append(sum(mape_score_train))\n",
    "\n",
    "            kf_avg_mse_test.append(sum(mse_score_test))\n",
    "            kf_avg_rmse_test.append(sum(rmse_score_test))\n",
    "            kf_avg_r2_test.append(sum(r2_score_test))\n",
    "            kf_avg_mape_test.append(sum(mape_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [0.3]\n",
      "Root Mean Squared Error: [0.54]\n",
      "R Squared Score is: [1.]\n",
      "Mean absolute error is: [0.36]\n",
      "TEST\n",
      "Mean Squared Error: [13.2]\n",
      "Root Mean Squared Error: [3.63]\n",
      "R Squared Score is: [0.999]\n",
      "Mean absolute error is: [2.32]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(kf_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(kf_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Gradint Boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "k_arr = [5]\n",
    "kf_avg_mse_train=[]\n",
    "kf_avg_rmse_train=[]\n",
    "kf_avg_r2_train=[]\n",
    "kf_avg_mape_train=[]\n",
    "\n",
    "kf_avg_mse_test=[]\n",
    "kf_avg_rmse_test=[]\n",
    "kf_avg_r2_test=[]\n",
    "kf_avg_mape_test=[]\n",
    "\n",
    "kf_fold_arr=[]\n",
    "\n",
    "for k in k_arr:\n",
    "\n",
    "            kf = KFold(n_splits=k, random_state=None)\n",
    "            gr = GradientBoostingRegressor(n_estimators=96, learning_rate=0.19104527704688606, loss='squared_error')\n",
    "\n",
    "            mse_score_train = []\n",
    "            rmse_score_train = []\n",
    "            r2_score_train = []\n",
    "            mape_score_train = []\n",
    "\n",
    "            mse_score_test = []\n",
    "            rmse_score_test = []\n",
    "            r2_score_test = []\n",
    "            mape_score_test = []\n",
    "\n",
    "            for i, (train_index , test_index) in enumerate(kf.split(dfx1)):\n",
    "                X_train , X_test = dfx1.iloc[train_index,:],dfx1.iloc[test_index,:]\n",
    "                y_train , y_test = dfy1.iloc[train_index] , dfy1.iloc[test_index]\n",
    "            y_train = np.array(y_train).ravel()    \n",
    "            gr.fit(X_train,y_train)\n",
    "            y_pred_train = gr.predict(X_train)\n",
    "            y_pred_test = gr.predict(X_test)\n",
    "            dump(gr,f'Gradint_boosting2_{i}.h5')\n",
    "            \n",
    "            mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "            mse_score_train.append(mse_train)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            rmse_score_train.append(rmse_train)\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_score_train.append(r2_train)\n",
    "            mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "            mape_score_train.append(mape_train)\n",
    "            mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "            mse_score_test.append(mse_test)\n",
    "            rmse_test = np.sqrt(mse_test)\n",
    "            rmse_score_test.append(rmse_test)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            r2_score_test.append(r2_test)\n",
    "            mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "            mape_score_test.append(mape_test)\n",
    "                \n",
    "            kf_fold_arr.append(k)\n",
    "\n",
    "            kf_avg_mse_train.append(sum(mse_score_train))\n",
    "            kf_avg_rmse_train.append(sum(rmse_score_train))\n",
    "            kf_avg_r2_train.append(sum(r2_score_train))\n",
    "            kf_avg_mape_train.append(sum(mape_score_train))\n",
    "\n",
    "            kf_avg_mse_test.append(sum(mse_score_test))\n",
    "            kf_avg_rmse_test.append(sum(rmse_score_test))\n",
    "            kf_avg_r2_test.append(sum(r2_score_test))\n",
    "            kf_avg_mape_test.append(sum(mape_score_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [6.5]\n",
      "Root Mean Squared Error: [2.55]\n",
      "R Squared Score is: [1.]\n",
      "Mean absolute error is: [1.87]\n",
      "TEST\n",
      "Mean Squared Error: [19.5]\n",
      "Root Mean Squared Error: [4.41]\n",
      "R Squared Score is: [0.998]\n",
      "Mean absolute error is: [3.29]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(kf_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(kf_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# XGBRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBRegressor\n",
    "k_arr = [5]\n",
    "kf_avg_mse_train=[]\n",
    "kf_avg_rmse_train=[]\n",
    "kf_avg_r2_train=[]\n",
    "kf_avg_mape_train=[]\n",
    "\n",
    "kf_avg_mse_test=[]\n",
    "kf_avg_rmse_test=[]\n",
    "kf_avg_r2_test=[]\n",
    "kf_avg_mape_test=[]\n",
    "\n",
    "kf_fold_arr=[]\n",
    "\n",
    "for k in k_arr:\n",
    "\n",
    "            kf = KFold(n_splits=k, random_state=None)\n",
    "            xb = XGBRegressor(n_estimators=98, learning_rate=0.12702909825189335, max_depth=5, random_state=42)\n",
    "\n",
    "            mse_score_train = []\n",
    "            rmse_score_train = []\n",
    "            r2_score_train = []\n",
    "            mape_score_train = []\n",
    "\n",
    "            mse_score_test = []\n",
    "            rmse_score_test = []\n",
    "            r2_score_test = []\n",
    "            mape_score_test = []\n",
    "\n",
    "            for i, (train_index , test_index) in enumerate(kf.split(dfx1)):\n",
    "                X_train , X_test = dfx1.iloc[train_index,:],dfx1.iloc[test_index,:]\n",
    "                y_train , y_test = dfy1.iloc[train_index] , dfy1.iloc[test_index]\n",
    "                \n",
    "            xb.fit(X_train,y_train)\n",
    "            y_pred_train = xb.predict(X_train)\n",
    "            y_pred_test = xb.predict(X_test)\n",
    "            dump(xb,f'XGBR2_{i}.h5')\n",
    "            \n",
    "            mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "            mse_score_train.append(mse_train)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            rmse_score_train.append(rmse_train)\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_score_train.append(r2_train)\n",
    "            mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "            mape_score_train.append(mape_train)\n",
    "            mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "            mse_score_test.append(mse_test)\n",
    "            rmse_test = np.sqrt(mse_test)\n",
    "            rmse_score_test.append(rmse_test)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            r2_score_test.append(r2_test)\n",
    "            mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "            mape_score_test.append(mape_test)\n",
    "                \n",
    "            kf_fold_arr.append(k)\n",
    "\n",
    "            \n",
    "            kf_avg_mse_train.append(sum(mse_score_train))\n",
    "            kf_avg_rmse_train.append(sum(rmse_score_train))\n",
    "            kf_avg_r2_train.append(sum(r2_score_train))\n",
    "            kf_avg_mape_train.append(sum(mape_score_train))\n",
    "\n",
    "            kf_avg_mse_test.append(sum(mse_score_test))\n",
    "            kf_avg_rmse_test.append(sum(rmse_score_test))\n",
    "            kf_avg_r2_test.append(sum(r2_score_test))\n",
    "            kf_avg_mape_test.append(sum(mape_score_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [2.8]\n",
      "Root Mean Squared Error: [1.67]\n",
      "R Squared Score is: [1.]\n",
      "Mean absolute error is: [1.16]\n",
      "TEST\n",
      "Mean Squared Error: [7.8]\n",
      "Root Mean Squared Error: [2.79]\n",
      "R Squared Score is: [0.999]\n",
      "Mean absolute error is: [2.05]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(kf_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(kf_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# RandomForestRegressor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "k_arr = [5]\n",
    "kf_avg_mse_train=[]\n",
    "kf_avg_rmse_train=[]\n",
    "kf_avg_r2_train=[]\n",
    "kf_avg_mape_train=[]\n",
    "\n",
    "kf_avg_mse_test=[]\n",
    "kf_avg_rmse_test=[]\n",
    "kf_avg_r2_test=[]\n",
    "kf_avg_mape_test=[]\n",
    "\n",
    "kf_fold_arr=[]\n",
    "\n",
    "for k in k_arr:\n",
    "\n",
    "            kf = KFold(n_splits=k, random_state=None)\n",
    "            gr =RandomForestRegressor(max_depth=18,n_estimators=93)\n",
    "\n",
    "            mse_score_train = []\n",
    "            rmse_score_train = []\n",
    "            r2_score_train = []\n",
    "            mape_score_train = []\n",
    "\n",
    "            mse_score_test = []\n",
    "            rmse_score_test = []\n",
    "            r2_score_test = []\n",
    "            mape_score_test = []\n",
    "\n",
    "            for i, (train_index , test_index) in enumerate(kf.split(dfx1)):\n",
    "                X_train , X_test = dfx1.iloc[train_index,:],dfx1.iloc[test_index,:]\n",
    "                y_train , y_test = dfy1.iloc[train_index] , dfy1.iloc[test_index]\n",
    "                \n",
    "            rt.fit(X_train,y_train)\n",
    "            y_pred_train = rt.predict(X_train)\n",
    "            y_pred_test = rt.predict(X_test)\n",
    "            dump(rt,f'RandomFores2_{i}.h5')\n",
    "            \n",
    "            mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "            mse_score_train.append(mse_train)\n",
    "            rmse_train = np.sqrt(mse_train)\n",
    "            rmse_score_train.append(rmse_train)\n",
    "            r2_train = r2_score(y_train, y_pred_train)\n",
    "            r2_score_train.append(r2_train)\n",
    "            mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "            mape_score_train.append(mape_train)\n",
    "            mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "            mse_score_test.append(mse_test)\n",
    "            rmse_test = np.sqrt(mse_test)\n",
    "            rmse_score_test.append(rmse_test)\n",
    "            r2_test = r2_score(y_test, y_pred_test)\n",
    "            r2_score_test.append(r2_test)\n",
    "            mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "            mape_score_test.append(mape_test)\n",
    "                \n",
    "            kf_fold_arr.append(k)\n",
    "\n",
    "        \n",
    "            kf_avg_mse_train.append(sum(mse_score_train))\n",
    "            kf_avg_rmse_train.append(sum(rmse_score_train))\n",
    "            kf_avg_r2_train.append(sum(r2_score_train))\n",
    "            kf_avg_mape_train.append(sum(mape_score_train))\n",
    "\n",
    "            kf_avg_mse_test.append(sum(mse_score_test))\n",
    "            kf_avg_rmse_test.append(sum(rmse_score_test))\n",
    "            kf_avg_r2_test.append(sum(r2_score_test))\n",
    "            kf_avg_mape_test.append(sum(mape_score_test))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [0.3]\n",
      "Root Mean Squared Error: [0.54]\n",
      "R Squared Score is: [1.]\n",
      "Mean absolute error is: [0.36]\n",
      "TEST\n",
      "Mean Squared Error: [12.8]\n",
      "Root Mean Squared Error: [3.58]\n",
      "R Squared Score is: [0.999]\n",
      "Mean absolute error is: [2.27]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(kf_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(kf_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(kf_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(kf_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(kf_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN 5 layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 612us/step - loss: 8246.1289\n",
      "Epoch 2/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 610us/step - loss: 652.2767\n",
      "Epoch 3/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 622us/step - loss: 302.4218\n",
      "Epoch 4/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 612us/step - loss: 202.7868\n",
      "Epoch 5/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 679us/step - loss: 149.9699\n",
      "Epoch 6/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - loss: 117.5193\n",
      "Epoch 7/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 601us/step - loss: 95.6193\n",
      "Epoch 8/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 597us/step - loss: 87.9162\n",
      "Epoch 9/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 606us/step - loss: 78.5610\n",
      "Epoch 10/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 604us/step - loss: 68.0413\n",
      "Epoch 11/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600us/step - loss: 65.7001\n",
      "Epoch 12/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 600us/step - loss: 56.3424\n",
      "Epoch 13/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 615us/step - loss: 55.1109\n",
      "Epoch 14/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 607us/step - loss: 50.6651\n",
      "Epoch 15/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599us/step - loss: 45.0286\n",
      "Epoch 16/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 616us/step - loss: 44.9434\n",
      "Epoch 17/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614us/step - loss: 41.4459\n",
      "Epoch 18/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599us/step - loss: 38.3093\n",
      "Epoch 19/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 623us/step - loss: 35.1741\n",
      "Epoch 20/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 610us/step - loss: 35.7556\n",
      "Epoch 21/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 621us/step - loss: 31.3079\n",
      "Epoch 22/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 596us/step - loss: 33.0352\n",
      "Epoch 23/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 604us/step - loss: 29.1417\n",
      "Epoch 24/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 619us/step - loss: 28.8026\n",
      "Epoch 25/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 621us/step - loss: 26.8639\n",
      "Epoch 26/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 584us/step - loss: 25.7751\n",
      "Epoch 27/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 735us/step - loss: 26.1667\n",
      "Epoch 28/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 684us/step - loss: 24.9937\n",
      "Epoch 29/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 608us/step - loss: 25.4957\n",
      "Epoch 30/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 629us/step - loss: 23.3012\n",
      "Epoch 31/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 609us/step - loss: 22.6449\n",
      "Epoch 32/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 579us/step - loss: 21.8507\n",
      "Epoch 33/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 580us/step - loss: 21.2890\n",
      "Epoch 34/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 648us/step - loss: 20.2521\n",
      "Epoch 35/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 588us/step - loss: 19.9217\n",
      "Epoch 36/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618us/step - loss: 19.7974\n",
      "Epoch 37/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 657us/step - loss: 19.2184\n",
      "Epoch 38/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 640us/step - loss: 18.3761\n",
      "Epoch 39/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 673us/step - loss: 18.3023\n",
      "Epoch 40/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 665us/step - loss: 18.3306\n",
      "Epoch 41/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 659us/step - loss: 17.4818\n",
      "Epoch 42/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 613us/step - loss: 17.8367\n",
      "Epoch 43/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 634us/step - loss: 18.0152\n",
      "Epoch 44/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 636us/step - loss: 17.1714\n",
      "Epoch 45/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599us/step - loss: 17.9217\n",
      "Epoch 46/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 643us/step - loss: 17.4215\n",
      "Epoch 47/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 623us/step - loss: 17.5575\n",
      "Epoch 48/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 599us/step - loss: 16.7439\n",
      "Epoch 49/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627us/step - loss: 17.2993\n",
      "Epoch 50/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 620us/step - loss: 17.4683\n",
      "Epoch 51/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 641us/step - loss: 17.4630\n",
      "Epoch 52/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 637us/step - loss: 17.5670\n",
      "Epoch 53/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 646us/step - loss: 16.4776\n",
      "Epoch 54/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 677us/step - loss: 16.1794\n",
      "Epoch 55/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 630us/step - loss: 15.7867\n",
      "Epoch 56/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627us/step - loss: 15.2948\n",
      "Epoch 57/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 613us/step - loss: 14.2286\n",
      "Epoch 58/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 630us/step - loss: 15.9151\n",
      "Epoch 59/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 618us/step - loss: 15.1817\n",
      "Epoch 60/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 630us/step - loss: 14.4467\n",
      "Epoch 61/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 621us/step - loss: 14.5676\n",
      "Epoch 62/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614us/step - loss: 14.2495\n",
      "Epoch 63/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 658us/step - loss: 13.4852\n",
      "Epoch 64/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627us/step - loss: 12.8366\n",
      "Epoch 65/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 647us/step - loss: 13.6689\n",
      "Epoch 66/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614us/step - loss: 12.6690\n",
      "Epoch 67/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 645us/step - loss: 12.7104\n",
      "Epoch 68/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 634us/step - loss: 12.7706\n",
      "Epoch 69/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 661us/step - loss: 12.0910\n",
      "Epoch 70/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 637us/step - loss: 12.1952\n",
      "Epoch 71/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 627us/step - loss: 12.0851\n",
      "Epoch 72/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - loss: 11.5998\n",
      "Epoch 73/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 642us/step - loss: 12.2237\n",
      "Epoch 74/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 658us/step - loss: 11.5951\n",
      "Epoch 75/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 651us/step - loss: 11.0967\n",
      "Epoch 76/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 614us/step - loss: 11.4775\n",
      "Epoch 77/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 649us/step - loss: 11.2494\n",
      "Epoch 78/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 626us/step - loss: 11.0655\n",
      "Epoch 79/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 632us/step - loss: 11.2824\n",
      "Epoch 80/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 662us/step - loss: 11.5760\n",
      "Epoch 81/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 645us/step - loss: 12.1396\n",
      "Epoch 82/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 633us/step - loss: 12.4737\n",
      "Epoch 83/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 637us/step - loss: 11.0634\n",
      "Epoch 84/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 656us/step - loss: 11.0764\n",
      "Epoch 85/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 615us/step - loss: 11.3181\n",
      "Epoch 86/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 585us/step - loss: 11.7634\n",
      "Epoch 87/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 601us/step - loss: 10.8135\n",
      "Epoch 88/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 576us/step - loss: 12.0303\n",
      "Epoch 89/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 631us/step - loss: 10.2322\n",
      "Epoch 90/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 675us/step - loss: 10.3757\n",
      "Epoch 91/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 606us/step - loss: 10.7606\n",
      "Epoch 92/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 605us/step - loss: 10.8309\n",
      "Epoch 93/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 633us/step - loss: 11.5113\n",
      "Epoch 94/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 648us/step - loss: 11.0097\n",
      "Epoch 95/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 633us/step - loss: 10.7164\n",
      "Epoch 96/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - loss: 11.1607\n",
      "Epoch 97/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 681us/step - loss: 10.3353\n",
      "Epoch 98/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 645us/step - loss: 10.3532\n",
      "Epoch 99/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 663us/step - loss: 10.0731\n",
      "Epoch 100/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 616us/step - loss: 11.2265\n",
      "Epoch 101/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 613us/step - loss: 10.3110\n",
      "Epoch 102/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 589us/step - loss: 11.9538\n",
      "Epoch 103/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 617us/step - loss: 10.4401\n",
      "Epoch 104/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 626us/step - loss: 10.7026\n",
      "Epoch 105/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - loss: 10.2904\n",
      "Epoch 106/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 649us/step - loss: 11.1869\n",
      "Epoch 107/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 638us/step - loss: 10.3427\n",
      "Epoch 108/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672us/step - loss: 9.9199\n",
      "Epoch 109/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 641us/step - loss: 10.2909\n",
      "Epoch 110/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 641us/step - loss: 9.9289\n",
      "Epoch 111/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 666us/step - loss: 10.4241\n",
      "Epoch 112/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 639us/step - loss: 10.0814\n",
      "Epoch 113/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 644us/step - loss: 9.9582\n",
      "Epoch 114/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 620us/step - loss: 10.0615\n",
      "Epoch 115/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 651us/step - loss: 10.1385\n",
      "Epoch 116/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 619us/step - loss: 10.8008\n",
      "Epoch 117/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 651us/step - loss: 10.6743\n",
      "Epoch 118/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 616us/step - loss: 9.5972\n",
      "Epoch 119/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 644us/step - loss: 10.6269\n",
      "Epoch 120/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 643us/step - loss: 9.9689\n",
      "Epoch 121/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 595us/step - loss: 9.2658\n",
      "Epoch 122/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 664us/step - loss: 8.9567\n",
      "Epoch 123/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 656us/step - loss: 10.4973\n",
      "Epoch 124/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 624us/step - loss: 9.5396\n",
      "Epoch 125/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 634us/step - loss: 9.9647\n",
      "Epoch 126/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 659us/step - loss: 10.1865\n",
      "Epoch 127/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 643us/step - loss: 9.5778\n",
      "Epoch 128/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 707us/step - loss: 9.7700\n",
      "Epoch 129/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 660us/step - loss: 9.3140\n",
      "Epoch 130/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 717us/step - loss: 9.0782\n",
      "Epoch 131/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 708us/step - loss: 9.5612\n",
      "Epoch 132/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 661us/step - loss: 9.4928\n",
      "Epoch 133/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 672us/step - loss: 8.8173\n",
      "Epoch 134/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 853us/step - loss: 9.3409\n",
      "Epoch 135/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 804us/step - loss: 8.4560\n",
      "Epoch 136/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 832us/step - loss: 8.2313\n",
      "Epoch 137/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 859us/step - loss: 8.9604\n",
      "Epoch 138/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 923us/step - loss: 9.5959\n",
      "Epoch 139/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 890us/step - loss: 8.6580\n",
      "Epoch 140/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 906us/step - loss: 9.3152\n",
      "Epoch 141/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 805us/step - loss: 8.5912\n",
      "Epoch 142/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 748us/step - loss: 8.8197\n",
      "Epoch 143/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 861us/step - loss: 8.5990\n",
      "Epoch 144/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 930us/step - loss: 8.6940\n",
      "Epoch 145/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 791us/step - loss: 8.2035\n",
      "Epoch 146/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 812us/step - loss: 8.6268\n",
      "Epoch 147/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 775us/step - loss: 9.0139\n",
      "Epoch 148/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 793us/step - loss: 7.9637\n",
      "Epoch 149/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 775us/step - loss: 8.5615\n",
      "Epoch 150/150\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 847us/step - loss: 8.9342\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 624us/step\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 709us/step\n"
     ]
    }
   ],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(dfx1, dfy1, test_size=0.25, shuffle=False)\n",
    "#k=2 to avoid modification to the code with the k-fold\n",
    "k=1\n",
    "\n",
    "neuron_Arr=[50]\n",
    "\n",
    "ann_avg_mse_train=[]\n",
    "ann_avg_rmse_train=[]\n",
    "ann_avg_r2_train=[]\n",
    "ann_avg_mape_train=[]\n",
    "\n",
    "ann_avg_mse_test=[]\n",
    "ann_avg_rmse_test=[]\n",
    "ann_avg_r2_test=[]\n",
    "ann_avg_mape_test=[]\n",
    "\n",
    "for neuron in neuron_Arr:\n",
    "\n",
    "          ANN = keras.Sequential([\n",
    "              keras.layers.Flatten(input_shape=(23,)),  # input layer (1)\n",
    "              keras.layers.Dense(neuron, activation='relu'),  # hidden layer (1)\n",
    "              keras.layers.Dense(neuron, activation='relu'),  # hidden layer (2)\n",
    "              keras.layers.Dense(neuron, activation='relu'),  # hidden layer (3)\n",
    "              keras.layers.Dense(neuron, activation='relu'),  # hidden layer (4)\n",
    "              keras.layers.Dense(neuron, activation='relu'),  # hidden layer (5)\n",
    "              keras.layers.Dense(1, activation='linear') # output layer (5)\n",
    "          ])\n",
    "\n",
    "\n",
    "          ANN.compile(optimizer='adam',loss=tf.keras.losses.MeanSquaredError())\n",
    "\n",
    "          keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                                        min_delta=0.1,\n",
    "                                        patience=3,\n",
    "                                        verbose=1, mode='auto')\n",
    "\n",
    "          mse_score_train = []\n",
    "          rmse_score_train = []\n",
    "          r2_score_train = []\n",
    "          mape_score_train = []\n",
    "\n",
    "          mse_score_test = []\n",
    "          rmse_score_test = []\n",
    "          r2_score_test = []\n",
    "          mape_score_test = []\n",
    "                  \n",
    "              \n",
    "          ANN.fit(X_train,y_train,epochs=150,shuffle=False)\n",
    "\n",
    "          y_pred_train = ANN.predict(X_train)\n",
    "          y_pred_test = ANN.predict(X_test)\n",
    "\n",
    "\n",
    "          mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "          mse_score_train.append(mse_train)\n",
    "          rmse_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "          rmse_score_train.append(rmse_train)\n",
    "          r2_train = r2_score(y_train, y_pred_train)\n",
    "          r2_score_train.append(r2_train)\n",
    "          mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "          mape_score_train.append(mape_train)\n",
    "\n",
    "          mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "          mse_score_test.append(mse_test)\n",
    "          rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n",
    "          rmse_score_test.append(rmse_test)\n",
    "          r2_test = r2_score(y_test, y_pred_test)\n",
    "          r2_score_test.append(r2_test)\n",
    "          mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "          mape_score_test.append(mape_test)\n",
    "              \n",
    "          ann_avg_mse_train.append(sum(mse_score_train)/k)\n",
    "          ann_avg_rmse_train.append(sum(rmse_score_train)/k)\n",
    "          ann_avg_r2_train.append(sum(r2_score_train)/k)\n",
    "          ann_avg_mape_train.append(sum(mape_score_train)/k)\n",
    "\n",
    "          ann_avg_mse_test.append(sum(mse_score_test)/k)\n",
    "          ann_avg_rmse_test.append(sum(rmse_score_test)/k)\n",
    "          ann_avg_r2_test.append(sum(r2_score_test)/k)\n",
    "          ann_avg_mape_test.append(sum(mape_score_test)/k)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "ANN.save('ANN5Layer.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with three hidden layers and with [50] neurons\n",
      "TRAINING\n",
      "Mean Squared Error: [9.4]\n",
      "Root Mean Squared Error: [3.06]\n",
      "R Squared Score is: [0.999]\n",
      "Mean absolute error is: [2.04]\n",
      "TEST\n",
      "Mean Squared Error: [21.6]\n",
      "Root Mean Squared Error: [4.65]\n",
      "R Squared Score is: [0.998]\n",
      "Mean absolute error is: [3.26]\n"
     ]
    }
   ],
   "source": [
    "print(\"Results with three hidden layers and with\", neuron_Arr, \"neurons\")\n",
    "\n",
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ann_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ann_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ann_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ann_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ann_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ann_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ann_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ann_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ANN 3 layer Op"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "ANN.save('ANN4.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Results with three hidden layers and with [50] neurons\n",
      "TRAINING\n",
      "Mean Squared Error: [33.5]\n",
      "Root Mean Squared Error: [5.79]\n",
      "R Squared Score is: [0.998]\n",
      "Mean absolute error is: [4.03]\n",
      "TEST\n",
      "Mean Squared Error: [30.4]\n",
      "Root Mean Squared Error: [5.52]\n",
      "R Squared Score is: [0.997]\n",
      "Mean absolute error is: [3.61]\n"
     ]
    }
   ],
   "source": [
    "print(\"Results with three hidden layers and with\", neuron_Arr, \"neurons\")\n",
    "\n",
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ann_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ann_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ann_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ann_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ann_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ann_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ann_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ann_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Attempt on ANN more"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmat\\AppData\\Roaming\\Python\\Python311\\site-packages\\keras\\src\\layers\\reshaping\\flatten.py:37: UserWarning: Do not pass an `input_shape`/`input_dim` argument to a layer. When using Sequential models, prefer using an `Input(shape)` object as the first layer in the model instead.\n",
      "  super().__init__(**kwargs)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 541us/step - loss: 11162.6309\n",
      "Epoch 2/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 556us/step - loss: 440.0934\n",
      "Epoch 3/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 563us/step - loss: 185.0396\n",
      "Epoch 4/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 138.5778\n",
      "Epoch 5/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 554us/step - loss: 111.8862\n",
      "Epoch 6/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 93.7191\n",
      "Epoch 7/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561us/step - loss: 77.7549\n",
      "Epoch 8/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 570us/step - loss: 69.0902\n",
      "Epoch 9/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533us/step - loss: 62.3858\n",
      "Epoch 10/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 551us/step - loss: 57.0735\n",
      "Epoch 11/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548us/step - loss: 53.5675\n",
      "Epoch 12/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 49.4558\n",
      "Epoch 13/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 526us/step - loss: 46.8460\n",
      "Epoch 14/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 526us/step - loss: 44.2332\n",
      "Epoch 15/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 534us/step - loss: 41.6323\n",
      "Epoch 16/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 39.2256\n",
      "Epoch 17/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 554us/step - loss: 37.5732\n",
      "Epoch 18/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 560us/step - loss: 36.1453\n",
      "Epoch 19/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548us/step - loss: 34.4294\n",
      "Epoch 20/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530us/step - loss: 32.7844\n",
      "Epoch 21/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 31.3052\n",
      "Epoch 22/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 569us/step - loss: 29.8179\n",
      "Epoch 23/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 567us/step - loss: 29.0252\n",
      "Epoch 24/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 546us/step - loss: 27.7950\n",
      "Epoch 25/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 26.9463\n",
      "Epoch 26/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543us/step - loss: 26.2396\n",
      "Epoch 27/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537us/step - loss: 25.4285\n",
      "Epoch 28/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542us/step - loss: 25.3635\n",
      "Epoch 29/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 547us/step - loss: 24.7815\n",
      "Epoch 30/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544us/step - loss: 24.3821\n",
      "Epoch 31/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537us/step - loss: 23.6092\n",
      "Epoch 32/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561us/step - loss: 23.2732\n",
      "Epoch 33/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515us/step - loss: 22.9055\n",
      "Epoch 34/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 516us/step - loss: 22.0261\n",
      "Epoch 35/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 21.9259\n",
      "Epoch 36/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537us/step - loss: 21.0700\n",
      "Epoch 37/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 517us/step - loss: 20.8752\n",
      "Epoch 38/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 20.3195\n",
      "Epoch 39/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542us/step - loss: 20.5838\n",
      "Epoch 40/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 518us/step - loss: 19.9861\n",
      "Epoch 41/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 19.1196\n",
      "Epoch 42/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542us/step - loss: 18.6115\n",
      "Epoch 43/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 538us/step - loss: 18.0499\n",
      "Epoch 44/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 546us/step - loss: 17.7324\n",
      "Epoch 45/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 520us/step - loss: 17.2771\n",
      "Epoch 46/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 16.9964\n",
      "Epoch 47/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528us/step - loss: 17.5387\n",
      "Epoch 48/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 512us/step - loss: 17.4271\n",
      "Epoch 49/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543us/step - loss: 16.7321\n",
      "Epoch 50/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 547us/step - loss: 16.5206\n",
      "Epoch 51/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522us/step - loss: 16.3284\n",
      "Epoch 52/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 15.7616\n",
      "Epoch 53/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 541us/step - loss: 15.1672\n",
      "Epoch 54/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 542us/step - loss: 14.5067\n",
      "Epoch 55/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 14.1153\n",
      "Epoch 56/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 563us/step - loss: 14.0365\n",
      "Epoch 57/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523us/step - loss: 13.9422\n",
      "Epoch 58/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 531us/step - loss: 13.6516\n",
      "Epoch 59/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528us/step - loss: 13.4961\n",
      "Epoch 60/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523us/step - loss: 13.4160\n",
      "Epoch 61/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514us/step - loss: 12.9880\n",
      "Epoch 62/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528us/step - loss: 12.8936\n",
      "Epoch 63/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 531us/step - loss: 13.2090\n",
      "Epoch 64/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 529us/step - loss: 13.2942\n",
      "Epoch 65/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523us/step - loss: 13.2479\n",
      "Epoch 66/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 12.7314\n",
      "Epoch 67/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 540us/step - loss: 12.7979\n",
      "Epoch 68/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 526us/step - loss: 13.0329\n",
      "Epoch 69/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 517us/step - loss: 13.2189\n",
      "Epoch 70/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 12.7869\n",
      "Epoch 71/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 533us/step - loss: 12.6512\n",
      "Epoch 72/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 513us/step - loss: 12.6139\n",
      "Epoch 73/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 551us/step - loss: 12.3561\n",
      "Epoch 74/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 545us/step - loss: 12.2046\n",
      "Epoch 75/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 12.0898\n",
      "Epoch 76/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 541us/step - loss: 12.0159\n",
      "Epoch 77/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 540us/step - loss: 11.9573\n",
      "Epoch 78/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 12.0032\n",
      "Epoch 79/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 560us/step - loss: 11.8360\n",
      "Epoch 80/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523us/step - loss: 12.1383\n",
      "Epoch 81/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 11.7007\n",
      "Epoch 82/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 525us/step - loss: 11.8053\n",
      "Epoch 83/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515us/step - loss: 11.8748\n",
      "Epoch 84/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 548us/step - loss: 12.0519\n",
      "Epoch 85/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 546us/step - loss: 11.7013\n",
      "Epoch 86/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 525us/step - loss: 12.2565\n",
      "Epoch 87/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 547us/step - loss: 11.7066\n",
      "Epoch 88/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 12.4231\n",
      "Epoch 89/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 12.0862\n",
      "Epoch 90/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 575us/step - loss: 12.1838\n",
      "Epoch 91/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 547us/step - loss: 12.0245\n",
      "Epoch 92/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 11.9840\n",
      "Epoch 93/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 11.9752\n",
      "Epoch 94/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 11.8288\n",
      "Epoch 95/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 556us/step - loss: 12.0750\n",
      "Epoch 96/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 537us/step - loss: 11.8601\n",
      "Epoch 97/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 520us/step - loss: 11.8168\n",
      "Epoch 98/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 547us/step - loss: 11.4348\n",
      "Epoch 99/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 11.5199\n",
      "Epoch 100/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 529us/step - loss: 11.5743\n",
      "Epoch 101/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535us/step - loss: 11.4247\n",
      "Epoch 102/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 521us/step - loss: 11.3390\n",
      "Epoch 103/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 557us/step - loss: 11.1768\n",
      "Epoch 104/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 11.1072\n",
      "Epoch 105/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 512us/step - loss: 10.9549\n",
      "Epoch 106/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 555us/step - loss: 10.6124\n",
      "Epoch 107/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515us/step - loss: 11.0249\n",
      "Epoch 108/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522us/step - loss: 11.0054\n",
      "Epoch 109/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544us/step - loss: 10.9972\n",
      "Epoch 110/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530us/step - loss: 10.8029\n",
      "Epoch 111/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 543us/step - loss: 10.8902\n",
      "Epoch 112/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 523us/step - loss: 10.4696\n",
      "Epoch 113/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519us/step - loss: 10.6507\n",
      "Epoch 114/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 515us/step - loss: 10.7679\n",
      "Epoch 115/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524us/step - loss: 10.3352\n",
      "Epoch 116/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 536us/step - loss: 10.3844\n",
      "Epoch 117/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 553us/step - loss: 10.5618\n",
      "Epoch 118/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 521us/step - loss: 10.2506\n",
      "Epoch 119/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 520us/step - loss: 10.4465\n",
      "Epoch 120/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 10.2534\n",
      "Epoch 121/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 535us/step - loss: 10.0176\n",
      "Epoch 122/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524us/step - loss: 10.0226\n",
      "Epoch 123/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 598us/step - loss: 9.9910 \n",
      "Epoch 124/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528us/step - loss: 10.1220\n",
      "Epoch 125/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 527us/step - loss: 9.6795\n",
      "Epoch 126/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 647us/step - loss: 9.6463\n",
      "Epoch 127/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 514us/step - loss: 9.4788\n",
      "Epoch 128/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519us/step - loss: 9.0145\n",
      "Epoch 129/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 558us/step - loss: 8.9876\n",
      "Epoch 130/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544us/step - loss: 9.0377\n",
      "Epoch 131/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 558us/step - loss: 9.1272\n",
      "Epoch 132/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 536us/step - loss: 9.2353\n",
      "Epoch 133/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 544us/step - loss: 9.7061\n",
      "Epoch 134/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 536us/step - loss: 9.3838\n",
      "Epoch 135/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 518us/step - loss: 8.9699\n",
      "Epoch 136/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 552us/step - loss: 9.4639\n",
      "Epoch 137/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 522us/step - loss: 9.1771\n",
      "Epoch 138/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 519us/step - loss: 8.9683\n",
      "Epoch 139/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 561us/step - loss: 9.3874\n",
      "Epoch 140/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 539us/step - loss: 8.6546\n",
      "Epoch 141/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 534us/step - loss: 8.7270\n",
      "Epoch 142/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 550us/step - loss: 9.0791\n",
      "Epoch 143/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 549us/step - loss: 8.9544\n",
      "Epoch 144/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 528us/step - loss: 8.7694\n",
      "Epoch 145/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530us/step - loss: 8.4808\n",
      "Epoch 146/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 546us/step - loss: 8.9424\n",
      "Epoch 147/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 524us/step - loss: 8.8931\n",
      "Epoch 148/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 530us/step - loss: 8.7700\n",
      "Epoch 149/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 532us/step - loss: 8.6058\n",
      "Epoch 150/150\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 526us/step - loss: 8.5520\n",
      "\u001b[1m5864/5864\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 398us/step\n",
      "\u001b[1m1466/1466\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 397us/step\n"
     ]
    }
   ],
   "source": [
    "k = 5\n",
    "kf = KFold(n_splits=k, random_state=None)\n",
    "\n",
    "\n",
    "ann_avg_mse_train=[]\n",
    "ann_avg_rmse_train=[]\n",
    "ann_avg_r2_train=[]\n",
    "ann_avg_mape_train=[]\n",
    "\n",
    "ann_avg_mse_test=[]\n",
    "ann_avg_rmse_test=[]\n",
    "ann_avg_r2_test=[]\n",
    "ann_avg_mape_test=[]\n",
    "\n",
    "mse_score_train = []\n",
    "rmse_score_train = []\n",
    "r2_score_train = []\n",
    "mape_score_train = []\n",
    "\n",
    "mse_score_test = []\n",
    "rmse_score_test = []\n",
    "r2_score_test = []\n",
    "mape_score_test = []\n",
    " \n",
    "for train_index , test_index in kf.split(dfx1):\n",
    "\n",
    "    X_train , X_test = dfx1.iloc[train_index,:],dfx1.iloc[test_index,:]\n",
    "    y_train , y_test = dfy1.iloc[train_index] , dfy1.iloc[test_index]\n",
    "     \n",
    "    \n",
    "ANN = keras.Sequential([\n",
    "    keras.layers.Flatten(input_shape=(23,)),  # input layer (1)\n",
    "    keras.layers.Dense(50, activation='relu'),  # hidden layer (1)\n",
    "    keras.layers.Dense(50, activation='relu'),  # hidden layer (2)\n",
    "    keras.layers.Dense(50, activation='relu'),  # hidden layer (3)\n",
    "    keras.layers.Dense(1, activation='linear') # output layer (7)\n",
    "])\n",
    "ANN.compile(optimizer='adam',loss=tf.keras.losses.MeanSquaredError())\n",
    "keras.callbacks.EarlyStopping(monitor='loss',\n",
    "                              min_delta=0.1,\n",
    "                              patience=3,\n",
    "                              verbose=1, mode='auto')        \n",
    "ANN.fit(X_train,y_train,epochs=150,shuffle=False)\n",
    "y_pred_train = ANN.predict(X_train)\n",
    "y_pred_test = ANN.predict(X_test)\n",
    " \n",
    "mse_train = metrics.mean_squared_error(y_train, y_pred_train)\n",
    "mse_score_train.append(mse_train)\n",
    "rmse_train = np.sqrt(metrics.mean_squared_error(y_train, y_pred_train))\n",
    "rmse_score_train.append(rmse_train)\n",
    "r2_train = r2_score(y_train, y_pred_train)\n",
    "r2_score_train.append(r2_train)\n",
    "mape_train = metrics.mean_absolute_error(y_train, y_pred_train)\n",
    "mape_score_train.append(mape_train)\n",
    "mse_test = metrics.mean_squared_error(y_test, y_pred_test)\n",
    "mse_score_test.append(mse_test)\n",
    "rmse_test = np.sqrt(metrics.mean_squared_error(y_test, y_pred_test))\n",
    "rmse_score_test.append(rmse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test)\n",
    "r2_score_test.append(r2_test)\n",
    "mape_test = metrics.mean_absolute_error(y_test, y_pred_test)\n",
    "mape_score_test.append(mape_test)\n",
    "     \n",
    "# ann_avg_mse_train.append(sum(mse_score_train)/k)\n",
    "# ann_avg_rmse_train.append(sum(rmse_score_train)/k)\n",
    "# ann_avg_r2_train.append(sum(r2_score_train)/k)\n",
    "# ann_avg_mape_train.append(sum(mape_score_train)/k)\n",
    "\n",
    "# ann_avg_mse_test.append(sum(mse_score_test)/k)\n",
    "# ann_avg_rmse_test.append(sum(rmse_score_test)/k)\n",
    "# ann_avg_r2_test.append(sum(r2_score_test)/k)\n",
    "# ann_avg_mape_test.append(sum(mape_score_test)/k)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [33.62737224463535]\n",
      "Root Mean Squared Error: [5.798911298221016]\n",
      "R Squared Score is: [0.9975623152764037]\n",
      "Mean absolute error is: [4.278098586150512]\n",
      "TEST\n",
      "Mean Squared Error: [19.811173931893514]\n",
      "Root Mean Squared Error: [4.450974492388551]\n",
      "R Squared Score is: [0.9981194977204345]\n",
      "Mean absolute error is: [3.291389072213347]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",mse_score_train)\n",
    "print(\"Root Mean Squared Error:\",rmse_score_train)\n",
    "print(\"R Squared Score is:\",r2_score_train)\n",
    "print(\"Mean absolute error is:\",mape_score_train)\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", (mse_score_test))\n",
    "print(\"Root Mean Squared Error:\",(rmse_score_test))\n",
    "print(\"R Squared Score is:\",r2_score_test)\n",
    "print(\"Mean absolute error is:\",mape_score_test)   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "ANN.save('ANN#LAyer.h5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensambling\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmat\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_voting.py:622: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.ensemble import VotingRegressor\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "from joblib import load\n",
    "from scikeras.wrappers import KerasRegressor  # Use scikeras for compatibility\n",
    "ens_avg_mse_train = []\n",
    "ens_avg_rmse_train = []\n",
    "ens_avg_r2_train = []\n",
    "ens_avg_mape_train = []\n",
    "ens_avg_mse_test = []\n",
    "ens_avg_rmse_test = []\n",
    "ens_avg_r2_test = []\n",
    "ens_avg_mape_test = []\n",
    "\n",
    "# Load pre-trained models\n",
    "dt = load('DecisionTreeRegressor_2_4.h5')\n",
    "Gb = load('Gradint_boosting2_4.h5')\n",
    "rf = load('RandomFores2_4.h5')\n",
    "xb  = load('XGBR2_4.h5')\n",
    "\n",
    "# Load ANN model architecture and weights separately\n",
    "def build_keras_model():\n",
    "    model = models.load_model(\"ANN5Layer.h5\")  # Load model architecture and weights\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')  # Reinitialize optimizer\n",
    "    return model\n",
    "\n",
    "# Wrap ANN model in KerasRegressor\n",
    "ann_regressor = KerasRegressor(model=build_keras_model)\n",
    "\n",
    "# Splitting dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfx1, dfy1, test_size=0.25, shuffle=False)\n",
    "\n",
    "# Voting Regressor with scikit-learn compatible models\n",
    "# voting_regressor = VotingRegressor(estimators=[('rf' , rf), ('DT', dt), ('GB', Gb),('ANN', ann_regressor)])\n",
    "voting_regressor = VotingRegressor(estimators=[('rf' , rf), ('DT', dt), ('GB', Gb),('XGB',xb)])\n",
    "\n",
    "voting_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions with Voting Regressor\n",
    "y_pred_train_vr = voting_regressor.predict(X_train)\n",
    "y_pred_test_vr = voting_regressor.predict(X_test)\n",
    "\n",
    "# Compute training metrics\n",
    "mse_train = mean_squared_error(y_train, y_pred_train_vr)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_pred_train_vr)\n",
    "mape_train = mean_absolute_error(y_train, y_pred_train_vr)\n",
    "\n",
    "# Compute testing metrics\n",
    "mse_test = mean_squared_error(y_test, y_pred_test_vr)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test_vr)\n",
    "mape_test = mean_absolute_error(y_test, y_pred_test_vr)\n",
    "\n",
    "# Store results\n",
    "ens_avg_mse_train.append(mse_train)\n",
    "ens_avg_rmse_train.append(rmse_train)\n",
    "ens_avg_r2_train.append(r2_train)\n",
    "ens_avg_mape_train.append(mape_train)\n",
    "ens_avg_mse_test.append(mse_test)\n",
    "ens_avg_rmse_test.append(rmse_test)\n",
    "ens_avg_r2_test.append(r2_test)\n",
    "ens_avg_mape_test.append(mape_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [0.9]\n",
      "Root Mean Squared Error: [0.94]\n",
      "R Squared Score is: [1.]\n",
      "Mean absolute error is: [0.69]\n",
      "TEST\n",
      "Mean Squared Error: [10.1]\n",
      "Root Mean Squared Error: [3.18]\n",
      "R Squared Score is: [0.999]\n",
      "Mean absolute error is: [2.2]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ens_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ens_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [270.1 832.8]\n",
      "Root Mean Squared Error: [16.43 28.86]\n",
      "R Squared Score is: [0.981 0.942]\n",
      "Mean absolute error is: [10.59 19.22]\n",
      "TEST\n",
      "Mean Squared Error: [1155.3  808.9]\n",
      "Root Mean Squared Error: [33.99 28.44]\n",
      "R Squared Score is: [0.88  0.916]\n",
      "Mean absolute error is: [23.59 22.05]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ens_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ens_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Stacking (Stacked Generalization)\n",
    "Combines multiple models (level-0 learners) using another model (meta-learner or level-1 model) to make final predictions.\n",
    "The base models make predictions, and the meta-learner combines them intelligently.\n",
    "Example: Using logistic regression as a meta-learner over random forests, SVMs, and neural networks."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmat\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_stacking.py:967: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stacking Model Performance:\n",
      "Train RMSE: 1.3842561695469606, R2: 0.9998660419107449, MAPE: 1.0308624568943712\n",
      "Test RMSE: 3.2141198785037703, R2: 0.9989304000460105, MAPE: 2.2172072718071125\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from joblib import load\n",
    "\n",
    "# Load dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfx1, dfy1, test_size=0.25, shuffle=False)\n",
    "\n",
    "# Load pre-trained models\n",
    "dt = load('DecisionTreeRegressor_2_4.h5')\n",
    "Gb = load('Gradint_boosting2_4.h5')\n",
    "rf = load('RandomFores2_4.h5')\n",
    "\n",
    "# Load ANN model\n",
    "\n",
    "def build_keras_model():\n",
    "    model = models.load_model(\"ANN5Layer.h5\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
    "    return model\n",
    "\n",
    "# Define Boosting models\n",
    "xgb_model = load('XGBR2_4.h5')\n",
    "\n",
    "# Train Boosting models\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Stacking ensemble with meta-learner\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=[('rf', rf), ('dt', dt), ('gb', Gb), ('xgb', xgb_model)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Train Stacking model\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_stack = stacking_regressor.predict(X_train)\n",
    "y_pred_test_stack = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Compute training metrics\n",
    "mse_train = mean_squared_error(y_train, y_pred_train_stack)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_pred_train_stack)\n",
    "mape_train = mean_absolute_error(y_train, y_pred_train_stack)\n",
    "\n",
    "# Compute testing metrics\n",
    "mse_test = mean_squared_error(y_test, y_pred_test_stack)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test_stack)\n",
    "mape_test = mean_absolute_error(y_test, y_pred_test_stack)\n",
    "\n",
    "# Store results\n",
    "ens_avg_mse_train.append(mse_train)\n",
    "ens_avg_rmse_train.append(rmse_train)\n",
    "ens_avg_r2_train.append(r2_train)\n",
    "ens_avg_mape_train.append(mape_train)\n",
    "ens_avg_mse_test.append(mse_test)\n",
    "ens_avg_rmse_test.append(rmse_test)\n",
    "ens_avg_r2_test.append(r2_test)\n",
    "ens_avg_mape_test.append(mape_test)\n",
    "\n",
    "print(\"Stacking Model Performance:\")\n",
    "print(f\"Train RMSE: {rmse_train}, R2: {r2_train}, MAPE: {mape_train}\")\n",
    "print(f\"Test RMSE: {rmse_test}, R2: {r2_test}, MAPE: {mape_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [0.9 1.9]\n",
      "Root Mean Squared Error: [0.94 1.38]\n",
      "R Squared Score is: [1. 1.]\n",
      "Mean absolute error is: [0.69 1.03]\n",
      "TEST\n",
      "Mean Squared Error: [10.1 10.3]\n",
      "Root Mean Squared Error: [3.18 3.21]\n",
      "R Squared Score is: [0.999 0.999]\n",
      "Mean absolute error is: [2.2  2.22]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ens_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ens_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StackingRegressor2.h5']"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(stacking_regressor, 'StackingRegressor2.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\dsmat\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_stacking.py:967: DataConversionWarning: A column-vector y was passed when a 1d array was expected. Please change the shape of y to (n_samples, ), for example using ravel().\n",
      "  y = column_or_1d(y, warn=True)\n",
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m6s\u001b[0m 698us/step - loss: 3.6727\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4398/4398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 674us/step - loss: 2.9139\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 456us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4398/4398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 648us/step - loss: 3.1423\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 462us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4398/4398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 647us/step - loss: 4.1271\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 428us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4398/4398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 661us/step - loss: 3.9137\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 458us/step\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m4398/4398\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 622us/step - loss: 4.0215\n",
      "\u001b[1m1100/1100\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 450us/step\n",
      "\u001b[1m5497/5497\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 402us/step\n",
      "\u001b[1m1833/1833\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 432us/step\n",
      "Stacking Model Performance:\n",
      "Train RMSE: 1.1802488058377425, R2: 0.9999026170003097, MAPE: 0.7844522271981581\n",
      "Test RMSE: 2.873356022728485, R2: 0.9991451771650619, MAPE: 2.0355734964236203\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingRegressor, StackingRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras import models\n",
    "from joblib import load\n",
    "\n",
    "# Load dataset\n",
    "X_train, X_test, y_train, y_test = train_test_split(dfx1, dfy1, test_size=0.25, shuffle=False)\n",
    "\n",
    "# Load pre-trained models\n",
    "dt = load('DecisionTreeRegressor_2_4.h5')\n",
    "Gb = load('Gradint_boosting2_4.h5')\n",
    "rf = load('RandomFores2_4.h5')\n",
    "\n",
    "# Load ANN model\n",
    "\n",
    "def build_keras_model():\n",
    "    model = models.load_model(\"ANN5Layer.h5\")\n",
    "    model.compile(optimizer=tf.keras.optimizers.Adam(), loss='mse')\n",
    "    return model\n",
    "\n",
    "ann = KerasRegressor(model=build_keras_model)\n",
    "\n",
    "\n",
    "# Define Boosting models\n",
    "xgb_model = load('XGBR2_4.h5')\n",
    "\n",
    "# Train Boosting models\n",
    "# xgb_model.fit(X_train, y_train)\n",
    "\n",
    "# Stacking ensemble with meta-learner\n",
    "stacking_regressor = StackingRegressor(\n",
    "    estimators=[('rf', rf), ('dt', dt), ('gb', Gb), ('xgb', xgb_model),('ANN' , ann)],\n",
    "    final_estimator=LinearRegression()\n",
    ")\n",
    "\n",
    "# Train Stacking model\n",
    "stacking_regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predictions\n",
    "y_pred_train_stack = stacking_regressor.predict(X_train)\n",
    "y_pred_test_stack = stacking_regressor.predict(X_test)\n",
    "\n",
    "# Compute training metrics\n",
    "mse_train = mean_squared_error(y_train, y_pred_train_stack)\n",
    "rmse_train = np.sqrt(mse_train)\n",
    "r2_train = r2_score(y_train, y_pred_train_stack)\n",
    "mape_train = mean_absolute_error(y_train, y_pred_train_stack)\n",
    "\n",
    "# Compute testing metrics\n",
    "mse_test = mean_squared_error(y_test, y_pred_test_stack)\n",
    "rmse_test = np.sqrt(mse_test)\n",
    "r2_test = r2_score(y_test, y_pred_test_stack)\n",
    "mape_test = mean_absolute_error(y_test, y_pred_test_stack)\n",
    "\n",
    "# Store results\n",
    "ens_avg_mse_train.append(mse_train)\n",
    "ens_avg_rmse_train.append(rmse_train)\n",
    "ens_avg_r2_train.append(r2_train)\n",
    "ens_avg_mape_train.append(mape_train)\n",
    "ens_avg_mse_test.append(mse_test)\n",
    "ens_avg_rmse_test.append(rmse_test)\n",
    "ens_avg_r2_test.append(r2_test)\n",
    "ens_avg_mape_test.append(mape_test)\n",
    "\n",
    "print(\"Stacking Model Performance:\")\n",
    "print(f\"Train RMSE: {rmse_train}, R2: {r2_train}, MAPE: {mape_train}\")\n",
    "print(f\"Test RMSE: {rmse_test}, R2: {r2_test}, MAPE: {mape_test}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TRAINING\n",
      "Mean Squared Error: [0.9 1.9 1.4]\n",
      "Root Mean Squared Error: [0.94 1.38 1.18]\n",
      "R Squared Score is: [1. 1. 1.]\n",
      "Mean absolute error is: [0.69 1.03 0.78]\n",
      "TEST\n",
      "Mean Squared Error: [10.1 10.3  8.3]\n",
      "Root Mean Squared Error: [3.18 3.21 2.87]\n",
      "R Squared Score is: [0.999 0.999 0.999]\n",
      "Mean absolute error is: [2.2  2.22 2.04]\n"
     ]
    }
   ],
   "source": [
    "print(\"TRAINING\")\n",
    "print(\"Mean Squared Error:\",np.around(np.array(ens_avg_mse_train),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_train),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_train),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_train),2))\n",
    "\n",
    "print(\"TEST\")\n",
    "print(\"Mean Squared Error:\", np.around(np.array(ens_avg_mse_test),1))\n",
    "print(\"Root Mean Squared Error:\",np.around(np.array(ens_avg_rmse_test),2))\n",
    "print(\"R Squared Score is:\", np.around(np.array(ens_avg_r2_test),3))\n",
    "print(\"Mean absolute error is:\", np.around(np.array(ens_avg_mape_test),2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['StackingRegressor_final.h5']"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dump(stacking_regressor, 'StackingRegressor_final.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
