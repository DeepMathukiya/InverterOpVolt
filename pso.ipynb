{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data = pd.read_csv('inverter Data Set.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfx1 = data.iloc[:,0:-3]\n",
    "#dfy1 = data_raw[['u_a_k-1','u_b_k-1','u_c_k-1']]\n",
    "dfy1 = data.iloc[:,-3:-2]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split    \n",
    "X_train, X_test1, y_train, y_test = train_test_split(dfx1, dfy1, test_size=0.30, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing GradientBoosting using PSO...\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[4], line 80\u001b[0m\n\u001b[0;32m     78\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m name, (model_class, param_bounds) \u001b[38;5;129;01min\u001b[39;00m models\u001b[38;5;241m.\u001b[39mitems():\n\u001b[0;32m     79\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimizing \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mname\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m using PSO...\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 80\u001b[0m     best_params, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mregression_pso\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_class\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparam_bounds\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     81\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBest Parameters: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_params\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, Best Loss: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_loss\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 65\u001b[0m, in \u001b[0;36mregression_pso\u001b[1;34m(model_class, param_bounds, n_particles, max_iter)\u001b[0m\n\u001b[0;32m     62\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m mean_squared_error(y_test, y_pred)\n\u001b[0;32m     64\u001b[0m bounds \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlist\u001b[39m(param_bounds\u001b[38;5;241m.\u001b[39mvalues())\n\u001b[1;32m---> 65\u001b[0m best_params, best_loss \u001b[38;5;241m=\u001b[39m \u001b[43mpso_optimize\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel_fitness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbounds\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mn_particles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmax_iter\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     67\u001b[0m best_params_dict \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;28mint\u001b[39m(val) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_bounds[key][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m val \u001b[38;5;28;01mfor\u001b[39;00m i, (key, val) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\u001b[38;5;28mzip\u001b[39m(param_bounds\u001b[38;5;241m.\u001b[39mkeys(), best_params))}\n\u001b[0;32m     68\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m best_params_dict, best_loss\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mpso_optimize\u001b[1;34m(func, dim, bounds, n_particles, max_iter, w, c1, c2, early_stop)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpso_optimize\u001b[39m(func, dim, bounds, n_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, c1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, c2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, early_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     19\u001b[0m     particles \u001b[38;5;241m=\u001b[39m [Particle(dim, bounds) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_particles)]\n\u001b[1;32m---> 20\u001b[0m     global_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mparticles\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     22\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m func(global_best\u001b[38;5;241m.\u001b[39mposition)\n\u001b[0;32m     23\u001b[0m     no_improve_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 20\u001b[0m, in \u001b[0;36mpso_optimize.<locals>.<lambda>\u001b[1;34m(p)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mpso_optimize\u001b[39m(func, dim, bounds, n_particles\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, max_iter\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m30\u001b[39m, w\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.5\u001b[39m, c1\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, c2\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1.5\u001b[39m, early_stop\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m):\n\u001b[0;32m     19\u001b[0m     particles \u001b[38;5;241m=\u001b[39m [Particle(dim, bounds) \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(n_particles)]\n\u001b[1;32m---> 20\u001b[0m     global_best \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mmin\u001b[39m(particles, key\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mlambda\u001b[39;00m p: \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43mp\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mposition\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m     22\u001b[0m     best_loss \u001b[38;5;241m=\u001b[39m func(global_best\u001b[38;5;241m.\u001b[39mposition)\n\u001b[0;32m     23\u001b[0m     no_improve_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[4], line 60\u001b[0m, in \u001b[0;36mregression_pso.<locals>.model_fitness\u001b[1;34m(params)\u001b[0m\n\u001b[0;32m     58\u001b[0m params \u001b[38;5;241m=\u001b[39m {key: \u001b[38;5;28mint\u001b[39m(val) \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(param_bounds[key][\u001b[38;5;241m0\u001b[39m], \u001b[38;5;28mint\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m val \u001b[38;5;28;01mfor\u001b[39;00m key, val \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(param_bounds\u001b[38;5;241m.\u001b[39mkeys(), params)}\n\u001b[0;32m     59\u001b[0m model \u001b[38;5;241m=\u001b[39m model_class(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams)\n\u001b[1;32m---> 60\u001b[0m \u001b[43mmodel\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     61\u001b[0m y_pred \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(X_test)\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m mean_squared_error(y_test, y_pred)\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_gb.py:784\u001b[0m, in \u001b[0;36mBaseGradientBoosting.fit\u001b[1;34m(self, X, y, sample_weight, monitor)\u001b[0m\n\u001b[0;32m    781\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_resize_state()\n\u001b[0;32m    783\u001b[0m \u001b[38;5;66;03m# fit the boosting stages\u001b[39;00m\n\u001b[1;32m--> 784\u001b[0m n_stages \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stages\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    785\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    786\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    787\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    788\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    789\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_rng\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    790\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    791\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    792\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight_val\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    793\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbegin_at_stage\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    794\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmonitor\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    795\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    797\u001b[0m \u001b[38;5;66;03m# change shape of arrays after fit (early-stopping or additional ests)\u001b[39;00m\n\u001b[0;32m    798\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m n_stages \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mestimators_\u001b[38;5;241m.\u001b[39mshape[\u001b[38;5;241m0\u001b[39m]:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_gb.py:880\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stages\u001b[1;34m(self, X, y, raw_predictions, sample_weight, random_state, X_val, y_val, sample_weight_val, begin_at_stage, monitor)\u001b[0m\n\u001b[0;32m    873\u001b[0m         initial_loss \u001b[38;5;241m=\u001b[39m factor \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_loss(\n\u001b[0;32m    874\u001b[0m             y_true\u001b[38;5;241m=\u001b[39my_oob_masked,\n\u001b[0;32m    875\u001b[0m             raw_prediction\u001b[38;5;241m=\u001b[39mraw_predictions[\u001b[38;5;241m~\u001b[39msample_mask],\n\u001b[0;32m    876\u001b[0m             sample_weight\u001b[38;5;241m=\u001b[39msample_weight_oob_masked,\n\u001b[0;32m    877\u001b[0m         )\n\u001b[0;32m    879\u001b[0m \u001b[38;5;66;03m# fit next stage of trees\u001b[39;00m\n\u001b[1;32m--> 880\u001b[0m raw_predictions \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_stage\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    881\u001b[0m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    882\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    883\u001b[0m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    884\u001b[0m \u001b[43m    \u001b[49m\u001b[43mraw_predictions\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    885\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    886\u001b[0m \u001b[43m    \u001b[49m\u001b[43msample_mask\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    887\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    888\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csc\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    889\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX_csr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mX_csr\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    890\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    892\u001b[0m \u001b[38;5;66;03m# track loss\u001b[39;00m\n\u001b[0;32m    893\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m do_oob:\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\ensemble\\_gb.py:490\u001b[0m, in \u001b[0;36mBaseGradientBoosting._fit_stage\u001b[1;34m(self, i, X, y, raw_predictions, sample_weight, sample_mask, random_state, X_csc, X_csr)\u001b[0m\n\u001b[0;32m    487\u001b[0m     sample_weight \u001b[38;5;241m=\u001b[39m sample_weight \u001b[38;5;241m*\u001b[39m sample_mask\u001b[38;5;241m.\u001b[39mastype(np\u001b[38;5;241m.\u001b[39mfloat64)\n\u001b[0;32m    489\u001b[0m X \u001b[38;5;241m=\u001b[39m X_csc \u001b[38;5;28;01mif\u001b[39;00m X_csc \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n\u001b[1;32m--> 490\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    491\u001b[0m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mneg_g_view\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mk\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\n\u001b[0;32m    492\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    494\u001b[0m \u001b[38;5;66;03m# update tree leaves\u001b[39;00m\n\u001b[0;32m    495\u001b[0m X_for_tree_update \u001b[38;5;241m=\u001b[39m X_csr \u001b[38;5;28;01mif\u001b[39;00m X_csr \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01melse\u001b[39;00m X\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\base.py:1474\u001b[0m, in \u001b[0;36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[1;34m(estimator, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1467\u001b[0m     estimator\u001b[38;5;241m.\u001b[39m_validate_params()\n\u001b[0;32m   1469\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[0;32m   1470\u001b[0m     skip_parameter_validation\u001b[38;5;241m=\u001b[39m(\n\u001b[0;32m   1471\u001b[0m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[0;32m   1472\u001b[0m     )\n\u001b[0;32m   1473\u001b[0m ):\n\u001b[1;32m-> 1474\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py:1377\u001b[0m, in \u001b[0;36mDecisionTreeRegressor.fit\u001b[1;34m(self, X, y, sample_weight, check_input)\u001b[0m\n\u001b[0;32m   1347\u001b[0m \u001b[38;5;129m@_fit_context\u001b[39m(prefer_skip_nested_validation\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m   1348\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfit\u001b[39m(\u001b[38;5;28mself\u001b[39m, X, y, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, check_input\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m):\n\u001b[0;32m   1349\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Build a decision tree regressor from the training set (X, y).\u001b[39;00m\n\u001b[0;32m   1350\u001b[0m \n\u001b[0;32m   1351\u001b[0m \u001b[38;5;124;03m    Parameters\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1374\u001b[0m \u001b[38;5;124;03m        Fitted estimator.\u001b[39;00m\n\u001b[0;32m   1375\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m-> 1377\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1378\u001b[0m \u001b[43m        \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1379\u001b[0m \u001b[43m        \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1380\u001b[0m \u001b[43m        \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1381\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcheck_input\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcheck_input\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1382\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1383\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "File \u001b[1;32m~\\AppData\\Roaming\\Python\\Python311\\site-packages\\sklearn\\tree\\_classes.py:472\u001b[0m, in \u001b[0;36mBaseDecisionTree._fit\u001b[1;34m(self, X, y, sample_weight, check_input, missing_values_in_feature_mask)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    462\u001b[0m     builder \u001b[38;5;241m=\u001b[39m BestFirstTreeBuilder(\n\u001b[0;32m    463\u001b[0m         splitter,\n\u001b[0;32m    464\u001b[0m         min_samples_split,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    469\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmin_impurity_decrease,\n\u001b[0;32m    470\u001b[0m     )\n\u001b[1;32m--> 472\u001b[0m \u001b[43mbuilder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbuild\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtree_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msample_weight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmissing_values_in_feature_mask\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    474\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_outputs_ \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m is_classifier(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    475\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_ \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mn_classes_[\u001b[38;5;241m0\u001b[39m]\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "\n",
    "# PSO Parameters\n",
    "class Particle:\n",
    "    def __init__(self, dim, bounds):\n",
    "        self.position = np.array([random.uniform(bounds[d][0], bounds[d][1]) for d in range(dim)])\n",
    "        self.velocity = np.zeros(dim)\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "def pso_optimize(func, dim, bounds, n_particles=10, max_iter=30, w=0.5, c1=1.5, c2=1.5, early_stop=5):\n",
    "    particles = [Particle(dim, bounds) for _ in range(n_particles)]\n",
    "    global_best = min(particles, key=lambda p: func(p.position))\n",
    "    \n",
    "    best_loss = func(global_best.position)\n",
    "    no_improve_count = 0\n",
    "    \n",
    "    for _ in range(max_iter):\n",
    "        for particle in particles:\n",
    "            score = func(particle.position)\n",
    "            if score < particle.best_score:\n",
    "                particle.best_score = score\n",
    "                particle.best_position = np.copy(particle.position)\n",
    "            \n",
    "            if score < best_loss:\n",
    "                global_best = particle\n",
    "                best_loss = score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "            \n",
    "            # Early stopping\n",
    "            if no_improve_count >= early_stop:\n",
    "                return global_best.position, best_loss\n",
    "            \n",
    "            # Update velocity and position\n",
    "            inertia = w * particle.velocity\n",
    "            cognitive = c1 * random.random() * (particle.best_position - particle.position)\n",
    "            social = c2 * random.random() * (global_best.position - particle.position)\n",
    "            particle.velocity = inertia + cognitive + social\n",
    "            particle.position += particle.velocity\n",
    "            \n",
    "    return global_best.position, best_loss\n",
    "\n",
    "# PSO for Regression Models\n",
    "def regression_pso(model_class, param_bounds, n_particles=5, max_iter=15):\n",
    "    y =np.array(Y_test).ravel()\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X_test1, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    def model_fitness(params):\n",
    "        params = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), params)}\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "    \n",
    "    bounds = list(param_bounds.values())\n",
    "    best_params, best_loss = pso_optimize(model_fitness, len(bounds), bounds, n_particles, max_iter)\n",
    "    \n",
    "    best_params_dict = {key: int(val) if isinstance(param_bounds[key][0], int) else val for i, (key, val) in enumerate(zip(param_bounds.keys(), best_params))}\n",
    "    return best_params_dict, best_loss\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models = {\n",
    "        # \"DecisionTree\": (DecisionTreeRegressor, {\"max_depth\": (1, 10), \"min_samples_split\": (2, 10)}),\n",
    "        \"GradientBoosting\": (GradientBoostingRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"XGBRegressor\": (XGBRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"RandomForest\": (RandomForestRegressor, {\"n_estimators\": (50, 100), \"max_depth\": (1, 10)})\n",
    "    }\n",
    "    \n",
    "    for name, (model_class, param_bounds) in models.items():\n",
    "        print(f\"Optimizing {name} using PSO...\")\n",
    "        best_params, best_loss = regression_pso(model_class, param_bounds)\n",
    "        print(f\"Best Parameters: {best_params}, Best Loss: {best_loss}\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing GradientBoosting using PSO...\n",
      "Best Parameters: {'n_estimators': 95, 'learning_rate': 0.18068326107174232}, Best Loss: 966.7597823129357\n",
      "\n",
      "Optimizing XGBRegressor using PSO...\n",
      "Best Parameters: {'n_estimators': 67, 'learning_rate': 0.11974981294015839}, Best Loss: 1506.2004467484524\n",
      "\n",
      "Optimizing RandomForest using PSO...\n",
      "Best Parameters: {'n_estimators': 67, 'max_depth': 8}, Best Loss: 2754.9013856246956\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "\n",
    "# Particle class for PSO\n",
    "class Particle:\n",
    "    def __init__(self, dim, bounds):\n",
    "        self.position = np.array([random.uniform(bounds[d][0], bounds[d][1]) for d in range(dim)])\n",
    "        self.velocity = np.zeros(dim)\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "def pso_optimize(func, dim, bounds, n_particles=7, max_iter=20, w=0.5, c1=1.5, c2=1.5, early_stop=5):\n",
    "    particles = [Particle(dim, bounds) for _ in range(n_particles)]\n",
    "    global_best = min(particles, key=lambda p: func(p.position))\n",
    "\n",
    "    best_loss = func(global_best.position)\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Parallel evaluation of particle fitness\n",
    "        scores = Parallel(n_jobs=-1)(delayed(func)(p.position) for p in particles)\n",
    "\n",
    "        for i, particle in enumerate(particles):\n",
    "            score = scores[i]\n",
    "\n",
    "            if score < particle.best_score:\n",
    "                particle.best_score = score\n",
    "                particle.best_position = np.copy(particle.position)\n",
    "\n",
    "            if score < best_loss:\n",
    "                global_best = particle\n",
    "                best_loss = score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improve_count >= early_stop:\n",
    "                return global_best.position, best_loss\n",
    "\n",
    "            # Update velocity and position\n",
    "            inertia = w * particle.velocity\n",
    "            cognitive = c1 * random.random() * (particle.best_position - particle.position)\n",
    "            social = c2 * random.random() * (global_best.position - particle.position)\n",
    "            particle.velocity = inertia + cognitive + social\n",
    "\n",
    "            # Update position and ensure it stays within bounds\n",
    "            particle.position += particle.velocity\n",
    "            particle.position = np.clip(particle.position, [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "    return global_best.position, best_loss\n",
    "\n",
    "# Function to optimize regression model using PSO\n",
    "def regression_pso(model_class, param_bounds, X_train, X_test, y_train, y_test, n_particles=7, max_iter=12):\n",
    "    def model_fitness(params):\n",
    "        # Convert parameters to correct types (int for discrete values)\n",
    "        params = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), params)}\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    bounds = list(param_bounds.values())\n",
    "    best_params, best_loss = pso_optimize(model_fitness, len(bounds), bounds, n_particles, max_iter)\n",
    "\n",
    "    # Convert best parameters to correct types\n",
    "    best_params_dict = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), best_params)}\n",
    "    return best_params_dict, best_loss\n",
    "\n",
    "# Generate sample regression data\n",
    "X, Y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models = {\n",
    "        \"GradientBoosting\": (GradientBoostingRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"XGBRegressor\": (XGBRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"RandomForest\": (RandomForestRegressor, {\"n_estimators\": (50, 100), \"max_depth\": (1, 10)})\n",
    "    }\n",
    "\n",
    "    for name, (model_class, param_bounds) in models.items():\n",
    "        print(f\"Optimizing {name} using PSO...\")\n",
    "        best_params, best_loss = regression_pso(model_class, param_bounds, X_train, X_test, Y_train, Y_test)\n",
    "        print(f\"Best Parameters: {best_params}, Best Loss: {best_loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing GradientBoosting using PSO...\n",
      "Best Parameters: {'n_estimators': 96, 'learning_rate': 0.19104527704688606}, Best Loss: 3.744757125329138\n",
      "\n",
      "Optimizing XGBRegressor using PSO...\n",
      "Best Parameters: {'n_estimators': 98, 'learning_rate': 0.12702909825189335}, Best Loss: 0.7779491026847029\n",
      "\n",
      "Optimizing RandomForest using PSO...\n",
      "Best Parameters: {'n_estimators': 82, 'max_depth': 9}, Best Loss: 1.8720939753949268\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "\n",
    "# Particle class for PSO\n",
    "class Particle:\n",
    "    def __init__(self, dim, bounds):\n",
    "        self.position = np.array([random.uniform(bounds[d][0], bounds[d][1]) for d in range(dim)])\n",
    "        self.velocity = np.zeros(dim)\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "def pso_optimize(func, dim, bounds, n_particles=7, max_iter=20, w=0.5, c1=1.5, c2=1.5, early_stop=5):\n",
    "    particles = [Particle(dim, bounds) for _ in range(n_particles)]\n",
    "    global_best = min(particles, key=lambda p: func(p.position))\n",
    "\n",
    "    best_loss = func(global_best.position)\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Parallel evaluation of particle fitness\n",
    "        scores = Parallel(n_jobs=-1)(delayed(func)(p.position) for p in particles)\n",
    "\n",
    "        for i, particle in enumerate(particles):\n",
    "            score = scores[i]\n",
    "\n",
    "            if score < particle.best_score:\n",
    "                particle.best_score = score\n",
    "                particle.best_position = np.copy(particle.position)\n",
    "\n",
    "            if score < best_loss:\n",
    "                global_best = particle\n",
    "                best_loss = score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improve_count >= early_stop:\n",
    "                return global_best.position, best_loss\n",
    "\n",
    "            # Update velocity and position\n",
    "            inertia = w * particle.velocity\n",
    "            cognitive = c1 * random.random() * (particle.best_position - particle.position)\n",
    "            social = c2 * random.random() * (global_best.position - particle.position)\n",
    "            particle.velocity = inertia + cognitive + social\n",
    "\n",
    "            # Update position and ensure it stays within bounds\n",
    "            particle.position += particle.velocity\n",
    "            particle.position = np.clip(particle.position, [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "    return global_best.position, best_loss\n",
    "\n",
    "# Function to optimize regression model using PSO\n",
    "def regression_pso(model_class, param_bounds, X_train, X_test, y_train, y_test, n_particles=7, max_iter=12):\n",
    "    def model_fitness(params):\n",
    "        # Convert parameters to correct types (int for discrete values)\n",
    "        params = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), params)}\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    bounds = list(param_bounds.values())\n",
    "    best_params, best_loss = pso_optimize(model_fitness, len(bounds), bounds, n_particles, max_iter)\n",
    "\n",
    "    # Convert best parameters to correct types\n",
    "    best_params_dict = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), best_params)}\n",
    "    return best_params_dict, best_loss\n",
    "\n",
    "# Generate sample regression data\n",
    "# X, Y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "y = np.array(y_test).ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_test1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models = {\n",
    "        \"GradientBoosting\": (GradientBoostingRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"XGBRegressor\": (XGBRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"RandomForest\": (RandomForestRegressor, {\"n_estimators\": (50, 100), \"max_depth\": (1, 10)})\n",
    "    }\n",
    "\n",
    "    for name, (model_class, param_bounds) in models.items():\n",
    "        print(f\"Optimizing {name} using PSO...\")\n",
    "        best_params, best_loss = regression_pso(model_class, param_bounds, X_train, X_test, Y_train, Y_test)\n",
    "        print(f\"Best Parameters: {best_params}, Best Loss: {best_loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Optimizing RandomForest using PSO...\n",
      "Best Parameters: {'n_estimators': 93, 'max_depth': 18}, Best Loss: 0.34348393013289963\n",
      "\n",
      "Optimizing DecisionTree using PSO...\n",
      "Best Parameters: {'max_depth': 16, 'min_samples_split': 2}, Best Loss: 0.9304527817225453\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import random\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.ensemble import GradientBoostingRegressor, RandomForestRegressor\n",
    "from xgboost import XGBRegressor\n",
    "from joblib import Parallel, delayed  # For parallel processing\n",
    "\n",
    "# Particle class for PSO\n",
    "class Particle:\n",
    "    def __init__(self, dim, bounds):\n",
    "        self.position = np.array([random.uniform(bounds[d][0], bounds[d][1]) for d in range(dim)])\n",
    "        self.velocity = np.zeros(dim)\n",
    "        self.best_position = np.copy(self.position)\n",
    "        self.best_score = float('inf')\n",
    "\n",
    "def pso_optimize(func, dim, bounds, n_particles=7, max_iter=20, w=0.5, c1=1.5, c2=1.5, early_stop=5):\n",
    "    particles = [Particle(dim, bounds) for _ in range(n_particles)]\n",
    "    global_best = min(particles, key=lambda p: func(p.position))\n",
    "\n",
    "    best_loss = func(global_best.position)\n",
    "    no_improve_count = 0\n",
    "\n",
    "    for _ in range(max_iter):\n",
    "        # Parallel evaluation of particle fitness\n",
    "        scores = Parallel(n_jobs=-1)(delayed(func)(p.position) for p in particles)\n",
    "\n",
    "        for i, particle in enumerate(particles):\n",
    "            score = scores[i]\n",
    "\n",
    "            if score < particle.best_score:\n",
    "                particle.best_score = score\n",
    "                particle.best_position = np.copy(particle.position)\n",
    "\n",
    "            if score < best_loss:\n",
    "                global_best = particle\n",
    "                best_loss = score\n",
    "                no_improve_count = 0\n",
    "            else:\n",
    "                no_improve_count += 1\n",
    "\n",
    "            # Early stopping\n",
    "            if no_improve_count >= early_stop:\n",
    "                return global_best.position, best_loss\n",
    "\n",
    "            # Update velocity and position\n",
    "            inertia = w * particle.velocity\n",
    "            cognitive = c1 * random.random() * (particle.best_position - particle.position)\n",
    "            social = c2 * random.random() * (global_best.position - particle.position)\n",
    "            particle.velocity = inertia + cognitive + social\n",
    "\n",
    "            # Update position and ensure it stays within bounds\n",
    "            particle.position += particle.velocity\n",
    "            particle.position = np.clip(particle.position, [b[0] for b in bounds], [b[1] for b in bounds])\n",
    "\n",
    "    return global_best.position, best_loss\n",
    "\n",
    "# Function to optimize regression model using PSO\n",
    "def regression_pso(model_class, param_bounds, X_train, X_test, y_train, y_test, n_particles=7, max_iter=12):\n",
    "    def model_fitness(params):\n",
    "        # Convert parameters to correct types (int for discrete values)\n",
    "        params = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), params)}\n",
    "        model = model_class(**params)\n",
    "        model.fit(X_train, y_train)\n",
    "        y_pred = model.predict(X_test)\n",
    "        return mean_squared_error(y_test, y_pred)\n",
    "\n",
    "    bounds = list(param_bounds.values())\n",
    "    best_params, best_loss = pso_optimize(model_fitness, len(bounds), bounds, n_particles, max_iter)\n",
    "\n",
    "    # Convert best parameters to correct types\n",
    "    best_params_dict = {key: int(val) if isinstance(param_bounds[key][0], int) else val for key, val in zip(param_bounds.keys(), best_params)}\n",
    "    return best_params_dict, best_loss\n",
    "\n",
    "# Generate sample regression data\n",
    "# X, Y = make_regression(n_samples=1000, n_features=10, noise=0.1, random_state=42)\n",
    "y = np.array(y_test).ravel()\n",
    "X_train, X_test, Y_train, Y_test = train_test_split(X_test1, y, test_size=0.2, random_state=42)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    models = {\n",
    "        # \"GradientBoosting\": (GradientBoostingRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        # \"XGBRegressor\": (XGBRegressor, {\"n_estimators\": (50, 100), \"learning_rate\": (0.05, 0.2)}),\n",
    "        \"RandomForest\": (RandomForestRegressor, {\"n_estimators\": (50, 100), \"max_depth\": (1, 20)}),\n",
    "        \n",
    "        \"DecisionTree\": (DecisionTreeRegressor, {\"max_depth\": (1, 20), \"min_samples_split\": (2, 10)}),\n",
    "    }\n",
    "\n",
    "    for name, (model_class, param_bounds) in models.items():\n",
    "        print(f\"Optimizing {name} using PSO...\")\n",
    "        best_params, best_loss = regression_pso(model_class, param_bounds, X_train, X_test, Y_train, Y_test)\n",
    "        print(f\"Best Parameters: {best_params}, Best Loss: {best_loss}\\n\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
